# Testing and Evaluation â€“ MinningBots

This document summarizes the testing and evaluation activities that I independently performed during the MinningBots project from a functional and user-oriented perspective.

These activities were carried out to better understand system behavior, evaluate response quality, and support demos, presentations, and product discussions. They do not represent all testing performed by the team.

---

## Purpose of the testing

The purpose of these testing activities was to:

- Understand how the system responded to different types of questions  
- Verify that answers were grounded in the uploaded documents  
- Evaluate clarity, usefulness, and consistency of responses  
- Identify functional issues from an end-user perspective  
- Support product discussions and demo preparation  

The focus was practical rather than technical.

---

## Testing approach

Testing was performed by interacting directly with the system as a user.

This included:
- Uploading documents  
- Asking questions related to document content  
- Observing how answers were generated  
- Checking whether document references were clear  
- Comparing responses against expected or ideal answers  

These tests were exploratory and iterative, based on curiosity and product understanding rather than formal test plans.

---

## Type of testing performed

The testing I performed can be described as:

- **Functional exploration**: checking how features behaved in practice  
- **User-oriented evaluation**: assessing whether responses made sense to non-technical users  
- **Content validation**: verifying alignment between answers and document content  
- **Quality observation**: noting clarity, structure, and tone of responses  

This helped identify strengths and areas that could be improved from a product perspective.

---

## Documentation of observations

Observations from testing were documented in notes and reports that included:

- Examples of correct and incorrect answers  
- Cases where responses were incomplete or unclear  
- Missing or confusing document references  
- General observations about response behavior and timing  

These documents were created to organize my own understanding and to support discussions and demos, not as formal defect reports.

---

## Evaluation criteria

From a functional perspective, the system was evaluated based on:

- Accuracy and relevance of answers  
- Clarity of language  
- Transparency of information sources  
- Usability for non-technical users  

These criteria were used informally to assess the overall quality of the user experience.

---

## Notes

Other team members conducted their own testing activities independently.  
This document reflects only the testing and evaluation work that I personally carried out and documented.
